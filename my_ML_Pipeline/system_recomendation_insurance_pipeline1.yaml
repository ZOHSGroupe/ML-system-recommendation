apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  generateName: insurance-recomendation-deploy-in-kubflow-
  annotations: {pipelines.kubeflow.org/kfp_sdk_version: 1.8.0, pipelines.kubeflow.org/pipeline_compilation_time: '2024-01-24T20:33:15.538487',
    pipelines.kubeflow.org/pipeline_spec: '{"description": "ML pipeline  de System
      recomendation", "inputs": [{"name": "data_path", "type": "String"}], "name":
      "Insurance Recomendation deploy in kubflow"}'}
  labels: {pipelines.kubeflow.org/kfp_sdk_version: 1.8.0}
spec:
  entrypoint: insurance-recomendation-deploy-in-kubflow
  templates:
  - name: clusturing-and-save-supervised-data
    container:
      args: []
      command:
      - sh
      - -c
      - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
        'pandas == 2.1.3' 'numpy == 1.26.2' 'scikit-learn==1.3.2' || PIP_DISABLE_PIP_VERSION_CHECK=1
        python3 -m pip install --quiet --no-warn-script-location 'pandas == 2.1.3'
        'numpy == 1.26.2' 'scikit-learn==1.3.2' --user) && "$0" "$@"
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - |
        def clusturing_and_save_supervised_data():
            from sklearn.cluster import KMeans
            from sklearn.preprocessing import StandardScaler
            import numpy as np
            import pandas as pd
            k_value =  3
            insurance = pd.read_csv('data/preparation_data_for_clusturing.csv')
            datascaler = StandardScaler()
            data_numeric= insurance[['Seniority','Power', 'Cylinder_capacity', 'Value_vehicle', 'N_doors', 'Weight']]
            data_insurance_scaled = datascaler.fit_transform(data_numeric)
            kmeans = KMeans(n_clusters=k_value, init='k-means++', max_iter=300, n_init=10, random_state=0)
            insurance['Cluster'] = kmeans.fit_predict(data_insurance_scaled)
            insurance.to_csv('data/unbalanced_supervised-lerning-data-insurance.csv', index=False)

        import argparse
        _parser = argparse.ArgumentParser(prog='Clusturing and save supervised data', description='')
        _parsed_args = vars(_parser.parse_args())

        _outputs = clusturing_and_save_supervised_data(**_parsed_args)
      image: python:3.9
      volumeMounts:
      - {mountPath: '{{inputs.parameters.data_path}}', name: systme-recomendation-vol}
    inputs:
      parameters:
      - {name: data_path}
      - {name: systme-recomendation-vol-name}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.0
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"implementation": {"container":
          {"args": [], "command": ["sh", "-c", "(PIP_DISABLE_PIP_VERSION_CHECK=1 python3
          -m pip install --quiet --no-warn-script-location ''pandas == 2.1.3'' ''numpy
          == 1.26.2'' ''scikit-learn==1.3.2'' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3
          -m pip install --quiet --no-warn-script-location ''pandas == 2.1.3'' ''numpy
          == 1.26.2'' ''scikit-learn==1.3.2'' --user) && \"$0\" \"$@\"", "sh", "-ec",
          "program_path=$(mktemp)\nprintf \"%s\" \"$0\" > \"$program_path\"\npython3
          -u \"$program_path\" \"$@\"\n", "def clusturing_and_save_supervised_data():\n    from
          sklearn.cluster import KMeans\n    from sklearn.preprocessing import StandardScaler\n    import
          numpy as np\n    import pandas as pd\n    k_value =  3\n    insurance =
          pd.read_csv(''data/preparation_data_for_clusturing.csv'')\n    datascaler
          = StandardScaler()\n    data_numeric= insurance[[''Seniority'',''Power'',
          ''Cylinder_capacity'', ''Value_vehicle'', ''N_doors'', ''Weight'']]\n    data_insurance_scaled
          = datascaler.fit_transform(data_numeric)\n    kmeans = KMeans(n_clusters=k_value,
          init=''k-means++'', max_iter=300, n_init=10, random_state=0)\n    insurance[''Cluster'']
          = kmeans.fit_predict(data_insurance_scaled)\n    insurance.to_csv(''data/unbalanced_supervised-lerning-data-insurance.csv'',
          index=False)\n\nimport argparse\n_parser = argparse.ArgumentParser(prog=''Clusturing
          and save supervised data'', description='''')\n_parsed_args = vars(_parser.parse_args())\n\n_outputs
          = clusturing_and_save_supervised_data(**_parsed_args)\n"], "image": "python:3.9"}},
          "name": "Clusturing and save supervised data"}', pipelines.kubeflow.org/component_ref: '{}',
        pipelines.kubeflow.org/max_cache_staleness: P0D}
    volumes:
    - name: systme-recomendation-vol
      persistentVolumeClaim: {claimName: '{{inputs.parameters.systme-recomendation-vol-name}}'}
  - name: evaluate-model-staking
    container:
      args: []
      command:
      - sh
      - -c
      - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
        'pandas == 2.1.3' 'numpy == 1.26.2' 'scikit-learn==1.3.2' || PIP_DISABLE_PIP_VERSION_CHECK=1
        python3 -m pip install --quiet --no-warn-script-location 'pandas == 2.1.3'
        'numpy == 1.26.2' 'scikit-learn==1.3.2' --user) && "$0" "$@"
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - "def evaluate_model_staking():\n    from sklearn.metrics import accuracy_score,\
        \ precision_score, recall_score, f1_score\n    from sklearn.metrics import\
        \ confusion_matrix\n    import numpy as np\n    import pandas as pd\n    import\
        \ pickle\n\n    with open(f'data/model.pkl','rb') as f:\n        model = pickle.load(f)\n\
        \    y_test = np.load(f'data/y_test.npy', allow_pickle=True) \n    X_test\
        \ = np.load(f'data/X_test.npy', allow_pickle=True)\n    y_pred = np.load(f'data/y_pred.npy',\
        \ allow_pickle=True)\n    accuracy = accuracy_score(y_test, y_pred)\n    precision\
        \ = precision_score(y_test, y_pred, average='weighted')\n    recall = recall_score(y_test,\
        \ y_pred, average='weighted')\n    f1 = f1_score(y_test, y_pred, average='weighted')\n\
        \    #matrix confision\n    matrix = confusion_matrix(y_test, y_pred)\n\n\
        \    print(f\"Stacking Classifier - Accuracy: {accuracy:.4f}, Precision: {precision:.4f},\
        \ Recall: {recall:.4f}, F1 Score: {f1:.4f}\")\n    print(matrix)\n\nimport\
        \ argparse\n_parser = argparse.ArgumentParser(prog='Evaluate model staking',\
        \ description='')\n_parsed_args = vars(_parser.parse_args())\n\n_outputs =\
        \ evaluate_model_staking(**_parsed_args)\n"
      image: python:3.9
      volumeMounts:
      - {mountPath: '{{inputs.parameters.data_path}}', name: systme-recomendation-vol}
    inputs:
      parameters:
      - {name: data_path}
      - {name: systme-recomendation-vol-name}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.0
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"implementation": {"container":
          {"args": [], "command": ["sh", "-c", "(PIP_DISABLE_PIP_VERSION_CHECK=1 python3
          -m pip install --quiet --no-warn-script-location ''pandas == 2.1.3'' ''numpy
          == 1.26.2'' ''scikit-learn==1.3.2'' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3
          -m pip install --quiet --no-warn-script-location ''pandas == 2.1.3'' ''numpy
          == 1.26.2'' ''scikit-learn==1.3.2'' --user) && \"$0\" \"$@\"", "sh", "-ec",
          "program_path=$(mktemp)\nprintf \"%s\" \"$0\" > \"$program_path\"\npython3
          -u \"$program_path\" \"$@\"\n", "def evaluate_model_staking():\n    from
          sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n    from
          sklearn.metrics import confusion_matrix\n    import numpy as np\n    import
          pandas as pd\n    import pickle\n\n    with open(f''data/model.pkl'',''rb'')
          as f:\n        model = pickle.load(f)\n    y_test = np.load(f''data/y_test.npy'',
          allow_pickle=True) \n    X_test = np.load(f''data/X_test.npy'', allow_pickle=True)\n    y_pred
          = np.load(f''data/y_pred.npy'', allow_pickle=True)\n    accuracy = accuracy_score(y_test,
          y_pred)\n    precision = precision_score(y_test, y_pred, average=''weighted'')\n    recall
          = recall_score(y_test, y_pred, average=''weighted'')\n    f1 = f1_score(y_test,
          y_pred, average=''weighted'')\n    #matrix confision\n    matrix = confusion_matrix(y_test,
          y_pred)\n\n    print(f\"Stacking Classifier - Accuracy: {accuracy:.4f},
          Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}\")\n    print(matrix)\n\nimport
          argparse\n_parser = argparse.ArgumentParser(prog=''Evaluate model staking'',
          description='''')\n_parsed_args = vars(_parser.parse_args())\n\n_outputs
          = evaluate_model_staking(**_parsed_args)\n"], "image": "python:3.9"}}, "name":
          "Evaluate model staking"}', pipelines.kubeflow.org/component_ref: '{}',
        pipelines.kubeflow.org/max_cache_staleness: P0D}
    volumes:
    - name: systme-recomendation-vol
      persistentVolumeClaim: {claimName: '{{inputs.parameters.systme-recomendation-vol-name}}'}
  - name: evaluate-models-level0
    container:
      args: []
      command:
      - sh
      - -c
      - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
        'pandas == 2.1.3' 'numpy == 1.26.2' 'scikit-learn==1.3.2' || PIP_DISABLE_PIP_VERSION_CHECK=1
        python3 -m pip install --quiet --no-warn-script-location 'pandas == 2.1.3'
        'numpy == 1.26.2' 'scikit-learn==1.3.2' --user) && "$0" "$@"
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - |
        def evaluate_models_level0():
            from sklearn.linear_model import LogisticRegression
            from sklearn.naive_bayes import GaussianNB
            from sklearn.svm import SVC
            from sklearn.neighbors import KNeighborsClassifier
            from sklearn.tree import DecisionTreeClassifier
            from sklearn.tree import ExtraTreeClassifier
            from sklearn.ensemble import RandomForestClassifier
            from sklearn.ensemble import BaggingClassifier
            from sklearn.ensemble import GradientBoostingClassifier
            from sklearn.ensemble import AdaBoostClassifier
            from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
            from sklearn.metrics import confusion_matrix
            import numpy as np
            import pandas as pd
            import pickle
            models = {
                'Logistic Regression': LogisticRegression(),
                'Naive Bayes': GaussianNB(),
                'Support Vector Machine': SVC(),
                'K-Nearest Neighbors': KNeighborsClassifier(),
                'Decision Tree': DecisionTreeClassifier(),
                'Random Forest': RandomForestClassifier(),
                'Bagging': BaggingClassifier(),
                'AdaBoost': AdaBoostClassifier(),
                'Gradient Boosting': GradientBoostingClassifier(),
                'Extra Trees': ExtraTreeClassifier(),
            }

            y_train = np.load(f'data/y_train.npy', allow_pickle=True)
            y_test = np.load(f'data/y_test.npy', allow_pickle=True)
            X_train = np.load(f'data/X_train.npy', allow_pickle=True)
            X_test = np.load(f'data/X_test.npy', allow_pickle=True)

            with open(f'data/minmaxscaler.pkl', 'rb') as f:
                scaler = pickle.load(f)

            X_train = scaler.transform(X_train)
            X_test = scaler.transform(X_test)
            print(X_train)
            for name, model in models.items():
                try:
                    model.fit(X_train, y_train)
                    y_pred = model.predict(X_test)

                    accuracy = accuracy_score(y_test, y_pred)
                    precision = precision_score(y_test, y_pred, average='weighted', zero_division=1)
                    recall = recall_score(y_test, y_pred, average='weighted')
                    f1 = f1_score(y_test, y_pred, average='weighted')

                    confusion_mat = confusion_matrix(y_test, y_pred)

                    print(f"{name} - Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}")
                    print(f"Confusion Matrix:\n{confusion_mat}\n{'-'*50}")
                except Exception as e:
                    print(f"Error occurred for {name}: {e}")

        import argparse
        _parser = argparse.ArgumentParser(prog='Evaluate models level0', description='')
        _parsed_args = vars(_parser.parse_args())

        _outputs = evaluate_models_level0(**_parsed_args)
      image: python:3.9
      volumeMounts:
      - {mountPath: '{{inputs.parameters.data_path}}', name: systme-recomendation-vol}
    inputs:
      parameters:
      - {name: data_path}
      - {name: systme-recomendation-vol-name}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.0
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"implementation": {"container":
          {"args": [], "command": ["sh", "-c", "(PIP_DISABLE_PIP_VERSION_CHECK=1 python3
          -m pip install --quiet --no-warn-script-location ''pandas == 2.1.3'' ''numpy
          == 1.26.2'' ''scikit-learn==1.3.2'' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3
          -m pip install --quiet --no-warn-script-location ''pandas == 2.1.3'' ''numpy
          == 1.26.2'' ''scikit-learn==1.3.2'' --user) && \"$0\" \"$@\"", "sh", "-ec",
          "program_path=$(mktemp)\nprintf \"%s\" \"$0\" > \"$program_path\"\npython3
          -u \"$program_path\" \"$@\"\n", "def evaluate_models_level0():\n    from
          sklearn.linear_model import LogisticRegression\n    from sklearn.naive_bayes
          import GaussianNB\n    from sklearn.svm import SVC\n    from sklearn.neighbors
          import KNeighborsClassifier\n    from sklearn.tree import DecisionTreeClassifier\n    from
          sklearn.tree import ExtraTreeClassifier\n    from sklearn.ensemble import
          RandomForestClassifier\n    from sklearn.ensemble import BaggingClassifier\n    from
          sklearn.ensemble import GradientBoostingClassifier\n    from sklearn.ensemble
          import AdaBoostClassifier\n    from sklearn.metrics import accuracy_score,
          precision_score, recall_score, f1_score\n    from sklearn.metrics import
          confusion_matrix\n    import numpy as np\n    import pandas as pd\n    import
          pickle\n    models = {\n        ''Logistic Regression'': LogisticRegression(),\n        ''Naive
          Bayes'': GaussianNB(),\n        ''Support Vector Machine'': SVC(),\n        ''K-Nearest
          Neighbors'': KNeighborsClassifier(),\n        ''Decision Tree'': DecisionTreeClassifier(),\n        ''Random
          Forest'': RandomForestClassifier(),\n        ''Bagging'': BaggingClassifier(),\n        ''AdaBoost'':
          AdaBoostClassifier(),\n        ''Gradient Boosting'': GradientBoostingClassifier(),\n        ''Extra
          Trees'': ExtraTreeClassifier(),\n    }\n\n    y_train = np.load(f''data/y_train.npy'',
          allow_pickle=True)\n    y_test = np.load(f''data/y_test.npy'', allow_pickle=True)\n    X_train
          = np.load(f''data/X_train.npy'', allow_pickle=True)\n    X_test = np.load(f''data/X_test.npy'',
          allow_pickle=True)\n\n    with open(f''data/minmaxscaler.pkl'', ''rb'')
          as f:\n        scaler = pickle.load(f)\n\n    X_train = scaler.transform(X_train)\n    X_test
          = scaler.transform(X_test)\n    print(X_train)\n    for name, model in models.items():\n        try:\n            model.fit(X_train,
          y_train)\n            y_pred = model.predict(X_test)\n\n            accuracy
          = accuracy_score(y_test, y_pred)\n            precision = precision_score(y_test,
          y_pred, average=''weighted'', zero_division=1)\n            recall = recall_score(y_test,
          y_pred, average=''weighted'')\n            f1 = f1_score(y_test, y_pred,
          average=''weighted'')\n\n            confusion_mat = confusion_matrix(y_test,
          y_pred)\n\n            print(f\"{name} - Accuracy: {accuracy:.4f}, Precision:
          {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}\")\n            print(f\"Confusion
          Matrix:\\n{confusion_mat}\\n{''-''*50}\")\n        except Exception as e:\n            print(f\"Error
          occurred for {name}: {e}\")\n\nimport argparse\n_parser = argparse.ArgumentParser(prog=''Evaluate
          models level0'', description='''')\n_parsed_args = vars(_parser.parse_args())\n\n_outputs
          = evaluate_models_level0(**_parsed_args)\n"], "image": "python:3.9"}}, "name":
          "Evaluate models level0"}', pipelines.kubeflow.org/component_ref: '{}',
        pipelines.kubeflow.org/max_cache_staleness: P0D}
    volumes:
    - name: systme-recomendation-vol
      persistentVolumeClaim: {claimName: '{{inputs.parameters.systme-recomendation-vol-name}}'}
  - name: insurance-recomendation-deploy-in-kubflow
    inputs:
      parameters:
      - {name: data_path}
    dag:
      tasks:
      - name: clusturing-and-save-supervised-data
        template: clusturing-and-save-supervised-data
        dependencies: [preparation-data-for-clusturing, systme-recomendation-vol]
        arguments:
          parameters:
          - {name: data_path, value: '{{inputs.parameters.data_path}}'}
          - {name: systme-recomendation-vol-name, value: '{{tasks.systme-recomendation-vol.outputs.parameters.systme-recomendation-vol-name}}'}
      - name: evaluate-model-staking
        template: evaluate-model-staking
        dependencies: [systme-recomendation-vol, train-model-staking]
        arguments:
          parameters:
          - {name: data_path, value: '{{inputs.parameters.data_path}}'}
          - {name: systme-recomendation-vol-name, value: '{{tasks.systme-recomendation-vol.outputs.parameters.systme-recomendation-vol-name}}'}
      - name: evaluate-models-level0
        template: evaluate-models-level0
        dependencies: [systme-recomendation-vol, train-test-split]
        arguments:
          parameters:
          - {name: data_path, value: '{{inputs.parameters.data_path}}'}
          - {name: systme-recomendation-vol-name, value: '{{tasks.systme-recomendation-vol.outputs.parameters.systme-recomendation-vol-name}}'}
      - name: preparation-data-for-clusturing
        template: preparation-data-for-clusturing
        dependencies: [systme-recomendation-vol]
        arguments:
          parameters:
          - {name: data_path, value: '{{inputs.parameters.data_path}}'}
          - {name: systme-recomendation-vol-name, value: '{{tasks.systme-recomendation-vol.outputs.parameters.systme-recomendation-vol-name}}'}
      - {name: systme-recomendation-vol, template: systme-recomendation-vol}
      - name: train-model-staking
        template: train-model-staking
        dependencies: [evaluate-models-level0, systme-recomendation-vol]
        arguments:
          parameters:
          - {name: data_path, value: '{{inputs.parameters.data_path}}'}
          - {name: systme-recomendation-vol-name, value: '{{tasks.systme-recomendation-vol.outputs.parameters.systme-recomendation-vol-name}}'}
      - name: train-test-split
        template: train-test-split
        dependencies: [systme-recomendation-vol, unbalanced-data-procissing-to-balanced]
        arguments:
          parameters:
          - {name: data_path, value: '{{inputs.parameters.data_path}}'}
          - {name: systme-recomendation-vol-name, value: '{{tasks.systme-recomendation-vol.outputs.parameters.systme-recomendation-vol-name}}'}
      - name: unbalanced-data-procissing-to-balanced
        template: unbalanced-data-procissing-to-balanced
        dependencies: [clusturing-and-save-supervised-data, systme-recomendation-vol]
        arguments:
          parameters:
          - {name: data_path, value: '{{inputs.parameters.data_path}}'}
          - {name: systme-recomendation-vol-name, value: '{{tasks.systme-recomendation-vol.outputs.parameters.systme-recomendation-vol-name}}'}
  - name: preparation-data-for-clusturing
    container:
      args: []
      command:
      - sh
      - -c
      - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
        'pandas == 2.1.3' 'numpy == 1.26.2' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3
        -m pip install --quiet --no-warn-script-location 'pandas == 2.1.3' 'numpy
        == 1.26.2' --user) && "$0" "$@"
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - |
        def preparation_data_for_clusturing():
            import numpy as np
            import pandas as pd
            insurance = pd.read_csv('data/unsupervised-lerning-data-insurance.csv', sep=';')
            columns_to_drops = ['ID', 'Date_start_contract', 'Date_last_renewal', 'Date_next_renewal', 'Date_birth', 'Date_driving_licence','Distribution_channel', 'Policies_in_force','Max_policies', 'Max_products','Date_lapse', 'Lapse', 'Payment', 'Premium', 'Cost_claims_year',
               'N_claims_year', 'N_claims_history', 'R_Claims_history', 'Area', 'Second_driver', 'Year_matriculation', 'Length'  ,'Type_risk' ]
            insurance = insurance.drop(columns= columns_to_drops, axis=1)
            insurance.drop_duplicates(inplace=True)
            insurance = insurance.dropna()
            insurance_type = {
            'P':1,
            'D':2
            }
            insurance['Type_fuel'] = insurance['Type_fuel'].map(insurance_type)
            insurance.to_csv('data/preparation_data_for_clusturing.csv', index=False)

        import argparse
        _parser = argparse.ArgumentParser(prog='Preparation data for clusturing', description='')
        _parsed_args = vars(_parser.parse_args())

        _outputs = preparation_data_for_clusturing(**_parsed_args)
      image: python:3.9
      volumeMounts:
      - {mountPath: '{{inputs.parameters.data_path}}', name: systme-recomendation-vol}
    inputs:
      parameters:
      - {name: data_path}
      - {name: systme-recomendation-vol-name}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.0
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"implementation": {"container":
          {"args": [], "command": ["sh", "-c", "(PIP_DISABLE_PIP_VERSION_CHECK=1 python3
          -m pip install --quiet --no-warn-script-location ''pandas == 2.1.3'' ''numpy
          == 1.26.2'' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet
          --no-warn-script-location ''pandas == 2.1.3'' ''numpy == 1.26.2'' --user)
          && \"$0\" \"$@\"", "sh", "-ec", "program_path=$(mktemp)\nprintf \"%s\" \"$0\"
          > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n", "def preparation_data_for_clusturing():\n    import
          numpy as np\n    import pandas as pd\n    insurance = pd.read_csv(''data/unsupervised-lerning-data-insurance.csv'',
          sep='';'')\n    columns_to_drops = [''ID'', ''Date_start_contract'', ''Date_last_renewal'',
          ''Date_next_renewal'', ''Date_birth'', ''Date_driving_licence'',''Distribution_channel'',
          ''Policies_in_force'',''Max_policies'', ''Max_products'',''Date_lapse'',
          ''Lapse'', ''Payment'', ''Premium'', ''Cost_claims_year'',\n       ''N_claims_year'',
          ''N_claims_history'', ''R_Claims_history'', ''Area'', ''Second_driver'',
          ''Year_matriculation'', ''Length''  ,''Type_risk'' ]\n    insurance = insurance.drop(columns=
          columns_to_drops, axis=1)\n    insurance.drop_duplicates(inplace=True)\n    insurance
          = insurance.dropna()\n    insurance_type = {\n    ''P'':1,\n    ''D'':2\n    }\n    insurance[''Type_fuel'']
          = insurance[''Type_fuel''].map(insurance_type)\n    insurance.to_csv(''data/preparation_data_for_clusturing.csv'',
          index=False)\n\nimport argparse\n_parser = argparse.ArgumentParser(prog=''Preparation
          data for clusturing'', description='''')\n_parsed_args = vars(_parser.parse_args())\n\n_outputs
          = preparation_data_for_clusturing(**_parsed_args)\n"], "image": "python:3.9"}},
          "name": "Preparation data for clusturing"}', pipelines.kubeflow.org/component_ref: '{}',
        pipelines.kubeflow.org/max_cache_staleness: P0D}
    volumes:
    - name: systme-recomendation-vol
      persistentVolumeClaim: {claimName: '{{inputs.parameters.systme-recomendation-vol-name}}'}
  - name: systme-recomendation-vol
    resource:
      action: create
      manifest: |
        apiVersion: v1
        kind: PersistentVolumeClaim
        metadata:
          name: '{{workflow.name}}-systme-recomendation-vol'
        spec:
          accessModes:
          - ReadWriteOnce
          resources:
            requests:
              storage: 2Gi
    outputs:
      parameters:
      - name: systme-recomendation-vol-manifest
        valueFrom: {jsonPath: '{}'}
      - name: systme-recomendation-vol-name
        valueFrom: {jsonPath: '{.metadata.name}'}
      - name: systme-recomendation-vol-size
        valueFrom: {jsonPath: '{.status.capacity.storage}'}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.0
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
  - name: train-model-staking
    container:
      args: []
      command:
      - sh
      - -c
      - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
        'pandas == 2.1.3' 'numpy == 1.26.2' 'scikit-learn==1.3.2' || PIP_DISABLE_PIP_VERSION_CHECK=1
        python3 -m pip install --quiet --no-warn-script-location 'pandas == 2.1.3'
        'numpy == 1.26.2' 'scikit-learn==1.3.2' --user) && "$0" "$@"
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - "def train_model_staking():\n    from sklearn.linear_model import LogisticRegression\n\
        \    from sklearn.naive_bayes import GaussianNB\n    from sklearn.svm import\
        \ SVC\n    from sklearn.neighbors import KNeighborsClassifier\n    from sklearn.ensemble\
        \ import RandomForestClassifier\n    from sklearn.ensemble import AdaBoostClassifier\n\
        \    import numpy as np\n    import pandas as pd\n    import pickle\n    from\
        \ sklearn.ensemble import StackingClassifier\n\n    y_train = np.load(f'data/y_train.npy',allow_pickle=True)\n\
        \    y_test = np.load(f'data/y_test.npy', allow_pickle=True)\n    X_train\
        \ = np.load(f'data/X_train.npy', allow_pickle=True)\n    X_test = np.load(f'data/X_test.npy',\
        \ allow_pickle=True)\n    with open(f'data/minmaxscaler.pkl','rb') as f:\n\
        \        scaler = pickle.load(f)\n    X_train = scaler.transform(X_train)\n\
        \    X_test = scaler.transform(X_test)\n\n    level0 = [\n    ('AdaBoost',\
        \ AdaBoostClassifier()),\n    ('Naive Bayes', GaussianNB()),\n    ('Support\
        \ Vector Machine', SVC()),\n    ('K-Nearest Neighbors', KNeighborsClassifier()),\n\
        \    ('Random Forest', RandomForestClassifier())\n    ]\n\n    # Define level1\
        \ classifier\n    level1 = LogisticRegression(max_iter=1000)\n\n    # Create\
        \ StackingClassifier\n    model = StackingClassifier(estimators=level0, final_estimator=level1,\
        \ cv=5)\n\n    # Fit the model\n    model.fit(X_train, y_train)\n\n    with\
        \ open('data/model.pkl', 'wb') as model_file:\n        pickle.dump(model,\
        \ model_file)\n\n    # Predictions\n    y_pred = model.predict(X_test)\n \
        \   np.save(f'data/y_pred.npy', y_pred) \n    print(y_pred)\n\nimport argparse\n\
        _parser = argparse.ArgumentParser(prog='Train model staking', description='')\n\
        _parsed_args = vars(_parser.parse_args())\n\n_outputs = train_model_staking(**_parsed_args)\n"
      image: python:3.9
      volumeMounts:
      - {mountPath: '{{inputs.parameters.data_path}}', name: systme-recomendation-vol}
    inputs:
      parameters:
      - {name: data_path}
      - {name: systme-recomendation-vol-name}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.0
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"implementation": {"container":
          {"args": [], "command": ["sh", "-c", "(PIP_DISABLE_PIP_VERSION_CHECK=1 python3
          -m pip install --quiet --no-warn-script-location ''pandas == 2.1.3'' ''numpy
          == 1.26.2'' ''scikit-learn==1.3.2'' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3
          -m pip install --quiet --no-warn-script-location ''pandas == 2.1.3'' ''numpy
          == 1.26.2'' ''scikit-learn==1.3.2'' --user) && \"$0\" \"$@\"", "sh", "-ec",
          "program_path=$(mktemp)\nprintf \"%s\" \"$0\" > \"$program_path\"\npython3
          -u \"$program_path\" \"$@\"\n", "def train_model_staking():\n    from sklearn.linear_model
          import LogisticRegression\n    from sklearn.naive_bayes import GaussianNB\n    from
          sklearn.svm import SVC\n    from sklearn.neighbors import KNeighborsClassifier\n    from
          sklearn.ensemble import RandomForestClassifier\n    from sklearn.ensemble
          import AdaBoostClassifier\n    import numpy as np\n    import pandas as
          pd\n    import pickle\n    from sklearn.ensemble import StackingClassifier\n\n    y_train
          = np.load(f''data/y_train.npy'',allow_pickle=True)\n    y_test = np.load(f''data/y_test.npy'',
          allow_pickle=True)\n    X_train = np.load(f''data/X_train.npy'', allow_pickle=True)\n    X_test
          = np.load(f''data/X_test.npy'', allow_pickle=True)\n    with open(f''data/minmaxscaler.pkl'',''rb'')
          as f:\n        scaler = pickle.load(f)\n    X_train = scaler.transform(X_train)\n    X_test
          = scaler.transform(X_test)\n\n    level0 = [\n    (''AdaBoost'', AdaBoostClassifier()),\n    (''Naive
          Bayes'', GaussianNB()),\n    (''Support Vector Machine'', SVC()),\n    (''K-Nearest
          Neighbors'', KNeighborsClassifier()),\n    (''Random Forest'', RandomForestClassifier())\n    ]\n\n    #
          Define level1 classifier\n    level1 = LogisticRegression(max_iter=1000)\n\n    #
          Create StackingClassifier\n    model = StackingClassifier(estimators=level0,
          final_estimator=level1, cv=5)\n\n    # Fit the model\n    model.fit(X_train,
          y_train)\n\n    with open(''data/model.pkl'', ''wb'') as model_file:\n        pickle.dump(model,
          model_file)\n\n    # Predictions\n    y_pred = model.predict(X_test)\n    np.save(f''data/y_pred.npy'',
          y_pred) \n    print(y_pred)\n\nimport argparse\n_parser = argparse.ArgumentParser(prog=''Train
          model staking'', description='''')\n_parsed_args = vars(_parser.parse_args())\n\n_outputs
          = train_model_staking(**_parsed_args)\n"], "image": "python:3.9"}}, "name":
          "Train model staking"}', pipelines.kubeflow.org/component_ref: '{}', pipelines.kubeflow.org/max_cache_staleness: P0D}
    volumes:
    - name: systme-recomendation-vol
      persistentVolumeClaim: {claimName: '{{inputs.parameters.systme-recomendation-vol-name}}'}
  - name: train-test-split
    container:
      args: []
      command:
      - sh
      - -c
      - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
        'pandas == 2.1.3' 'numpy == 1.26.2' 'scikit-learn==1.3.2' || PIP_DISABLE_PIP_VERSION_CHECK=1
        python3 -m pip install --quiet --no-warn-script-location 'pandas == 2.1.3'
        'numpy == 1.26.2' 'scikit-learn==1.3.2' --user) && "$0" "$@"
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - |
        def train_test_split():
            from sklearn.model_selection import train_test_split
            from sklearn.preprocessing import MinMaxScaler
            import numpy as np
            import pandas as pd
            import pickle
            balanced_insurance = pd.read_csv('data/balanced_supervised-lerning-data-insurance.csv')
            X = balanced_insurance.drop('Cluster', axis=1)  # Exclude the 'Cluster' column
            y = balanced_insurance['Cluster']
            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
            scaler = MinMaxScaler()
            X_train = scaler.fit_transform(X_train)
            X_test = scaler.transform(X_test)
            np.save(f'data/y_train.npy', y_train)
            np.save(f'data/y_test.npy', y_test)
            np.save(f'data/X_train.npy', X_train)
            np.save(f'data/X_test.npy', X_test)
            with open('data/minmaxscaler.pkl', 'wb') as scaler_file:
                pickle.dump(scaler, scaler_file)

            print("\n---- X_train ----")
            print("\n")
            print(X_train)

            print("\n---- X_test ----")
            print("\n")
            print(X_test)

            print("\n---- y_train ----")
            print("\n")
            print(y_train)

            print("\n---- y_test ----")
            print("\n")
            print(y_test)

        import argparse
        _parser = argparse.ArgumentParser(prog='Train test split', description='')
        _parsed_args = vars(_parser.parse_args())

        _outputs = train_test_split(**_parsed_args)
      image: python:3.9
      volumeMounts:
      - {mountPath: '{{inputs.parameters.data_path}}', name: systme-recomendation-vol}
    inputs:
      parameters:
      - {name: data_path}
      - {name: systme-recomendation-vol-name}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.0
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"implementation": {"container":
          {"args": [], "command": ["sh", "-c", "(PIP_DISABLE_PIP_VERSION_CHECK=1 python3
          -m pip install --quiet --no-warn-script-location ''pandas == 2.1.3'' ''numpy
          == 1.26.2'' ''scikit-learn==1.3.2'' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3
          -m pip install --quiet --no-warn-script-location ''pandas == 2.1.3'' ''numpy
          == 1.26.2'' ''scikit-learn==1.3.2'' --user) && \"$0\" \"$@\"", "sh", "-ec",
          "program_path=$(mktemp)\nprintf \"%s\" \"$0\" > \"$program_path\"\npython3
          -u \"$program_path\" \"$@\"\n", "def train_test_split():\n    from sklearn.model_selection
          import train_test_split\n    from sklearn.preprocessing import MinMaxScaler\n    import
          numpy as np\n    import pandas as pd\n    import pickle\n    balanced_insurance
          = pd.read_csv(''data/balanced_supervised-lerning-data-insurance.csv'')\n    X
          = balanced_insurance.drop(''Cluster'', axis=1)  # Exclude the ''Cluster''
          column\n    y = balanced_insurance[''Cluster'']\n    X_train, X_test, y_train,
          y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n    scaler
          = MinMaxScaler()\n    X_train = scaler.fit_transform(X_train)\n    X_test
          = scaler.transform(X_test)\n    np.save(f''data/y_train.npy'', y_train)\n    np.save(f''data/y_test.npy'',
          y_test)\n    np.save(f''data/X_train.npy'', X_train)\n    np.save(f''data/X_test.npy'',
          X_test)\n    with open(''data/minmaxscaler.pkl'', ''wb'') as scaler_file:\n        pickle.dump(scaler,
          scaler_file)\n\n    print(\"\\n---- X_train ----\")\n    print(\"\\n\")\n    print(X_train)\n\n    print(\"\\n----
          X_test ----\")\n    print(\"\\n\")\n    print(X_test)\n\n    print(\"\\n----
          y_train ----\")\n    print(\"\\n\")\n    print(y_train)\n\n    print(\"\\n----
          y_test ----\")\n    print(\"\\n\")\n    print(y_test)\n\nimport argparse\n_parser
          = argparse.ArgumentParser(prog=''Train test split'', description='''')\n_parsed_args
          = vars(_parser.parse_args())\n\n_outputs = train_test_split(**_parsed_args)\n"],
          "image": "python:3.9"}}, "name": "Train test split"}', pipelines.kubeflow.org/component_ref: '{}',
        pipelines.kubeflow.org/max_cache_staleness: P0D}
    volumes:
    - name: systme-recomendation-vol
      persistentVolumeClaim: {claimName: '{{inputs.parameters.systme-recomendation-vol-name}}'}
  - name: unbalanced-data-procissing-to-balanced
    container:
      args: []
      command:
      - sh
      - -c
      - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
        'pandas == 2.1.3' 'numpy == 1.26.2' 'scikit-learn==1.3.2' || PIP_DISABLE_PIP_VERSION_CHECK=1
        python3 -m pip install --quiet --no-warn-script-location 'pandas == 2.1.3'
        'numpy == 1.26.2' 'scikit-learn==1.3.2' --user) && "$0" "$@"
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - |
        def unbalanced_data_procissing_to_balanced():
            import numpy as np
            import pandas as pd
            from sklearn.utils import resample
            insurance = pd.read_csv('data/unbalanced_supervised-lerning-data-insurance.csv')
            print(insurance['Cluster'].value_counts())
            min_observation_count = insurance['Cluster'].value_counts().min()

            balanced_dfs = []

            for cluster_label, group in insurance.groupby('Cluster'):

                balanced_cluster = resample(group, replace=False, n_samples=min_observation_count, random_state=42)

                balanced_dfs.append(balanced_cluster)

            balanced_insurance = pd.concat(balanced_dfs, ignore_index=True)

            print(balanced_insurance['Cluster'].value_counts())
            print(balanced_insurance.shape)
            print(balanced_insurance.info())
            balanced_insurance.to_csv('data/balanced_supervised-lerning-data-insurance.csv', index=False)

        import argparse
        _parser = argparse.ArgumentParser(prog='Unbalanced data procissing to balanced', description='')
        _parsed_args = vars(_parser.parse_args())

        _outputs = unbalanced_data_procissing_to_balanced(**_parsed_args)
      image: python:3.9
      volumeMounts:
      - {mountPath: '{{inputs.parameters.data_path}}', name: systme-recomendation-vol}
    inputs:
      parameters:
      - {name: data_path}
      - {name: systme-recomendation-vol-name}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.0
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"implementation": {"container":
          {"args": [], "command": ["sh", "-c", "(PIP_DISABLE_PIP_VERSION_CHECK=1 python3
          -m pip install --quiet --no-warn-script-location ''pandas == 2.1.3'' ''numpy
          == 1.26.2'' ''scikit-learn==1.3.2'' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3
          -m pip install --quiet --no-warn-script-location ''pandas == 2.1.3'' ''numpy
          == 1.26.2'' ''scikit-learn==1.3.2'' --user) && \"$0\" \"$@\"", "sh", "-ec",
          "program_path=$(mktemp)\nprintf \"%s\" \"$0\" > \"$program_path\"\npython3
          -u \"$program_path\" \"$@\"\n", "def unbalanced_data_procissing_to_balanced():\n    import
          numpy as np\n    import pandas as pd\n    from sklearn.utils import resample\n    insurance
          = pd.read_csv(''data/unbalanced_supervised-lerning-data-insurance.csv'')\n    print(insurance[''Cluster''].value_counts())\n    min_observation_count
          = insurance[''Cluster''].value_counts().min()\n\n    balanced_dfs = []\n\n    for
          cluster_label, group in insurance.groupby(''Cluster''):\n\n        balanced_cluster
          = resample(group, replace=False, n_samples=min_observation_count, random_state=42)\n\n        balanced_dfs.append(balanced_cluster)\n\n    balanced_insurance
          = pd.concat(balanced_dfs, ignore_index=True)\n\n    print(balanced_insurance[''Cluster''].value_counts())\n    print(balanced_insurance.shape)\n    print(balanced_insurance.info())\n    balanced_insurance.to_csv(''data/balanced_supervised-lerning-data-insurance.csv'',
          index=False)\n\nimport argparse\n_parser = argparse.ArgumentParser(prog=''Unbalanced
          data procissing to balanced'', description='''')\n_parsed_args = vars(_parser.parse_args())\n\n_outputs
          = unbalanced_data_procissing_to_balanced(**_parsed_args)\n"], "image": "python:3.9"}},
          "name": "Unbalanced data procissing to balanced"}', pipelines.kubeflow.org/component_ref: '{}',
        pipelines.kubeflow.org/max_cache_staleness: P0D}
    volumes:
    - name: systme-recomendation-vol
      persistentVolumeClaim: {claimName: '{{inputs.parameters.systme-recomendation-vol-name}}'}
  arguments:
    parameters:
    - {name: data_path}
  serviceAccountName: pipeline-runner
